# -*- coding: utf-8 -*-
import pathlib
import re
import time
from io import BytesIO
from typing import Optional

import streamlit as st
import streamlit.components.v1 as components
import pandas as pd
import requests

from web_fonts import inject_noto_sans_kr


st.set_page_config(page_title="ì˜ë™ì™€ì¸ ë¹…ë°ì´í„° í™ˆ", layout="wide")
inject_noto_sans_kr()

st.title("ğŸ· ì˜ë™ì™€ì¸ ë¹…ë°ì´í„° í™ˆí˜ì´ì§€")
st.caption("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ì›í•˜ëŠ” ë¶„ì„ í˜ì´ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”.")

with st.sidebar:
    st.header("ë©”ë‰´")
    menu = st.radio("ì´ë™", ["í™ˆ", "ê´€ë ¨ë‰´ìŠ¤"], index=0, label_visibility="collapsed")

    # ê´€ë ¨ë‰´ìŠ¤ ì„¤ì •(ë‰´ìŠ¤ ë©”ë‰´ì—ì„œ ì‚¬ìš©)
    st.divider()
    st.header("ê´€ë ¨ë‰´ìŠ¤")
    news_sheet_url = st.text_input("ê´€ë ¨ë‰´ìŠ¤ ì‹œíŠ¸ URL", value="https://docs.google.com/spreadsheets/d/1JsksLQuGqXuL7RGacqZyEmHxCrTIMHOVwlAIM32HUAo/edit?usp=sharing")
    news_query = st.text_input("ë‰´ìŠ¤ ê²€ìƒ‰(ì œëª©)", value="")
    max_items = st.slider("í‘œì‹œ ê°œìˆ˜", 5, 100, 20, 5)
    if st.button("ê´€ë ¨ë‰´ìŠ¤ ìƒˆë¡œê³ ì¹¨(ìºì‹œ ì‚­ì œ)", width="stretch"):
        st.cache_data.clear()

if menu == "í™ˆ":
    intro_path = pathlib.Path(__file__).with_name("intro.html")
    if intro_path.exists():
        html = intro_path.read_text(encoding="utf-8")
        components.html(html, height=950, scrolling=True)
    else:
        st.warning("intro.html íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    st.divider()

    st.subheader("ë°”ë¡œê°€ê¸°")
    st.write("ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ê° ë¶„ì„ í˜ì´ì§€ë¡œ ì´ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Streamlit ë²„ì „ì— ë”°ë¼ ë²„íŠ¼ ì´ë™ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ“ˆ ë„¤ì´ë²„ DataLab ì–¸ê¸‰ëŸ‰ íŠ¸ë Œë“œ", width="stretch"):
            try:
                st.switch_page("pages/01_naver_datalab_trend.py")
            except Exception:
                st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ 'ë„¤ì´ë²„ DataLab ì–¸ê¸‰ëŸ‰ íŠ¸ë Œë“œ' í˜ì´ì§€ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        if st.button("ğŸ“Š ë„¤ì´ë²„ DataLab ë¹„êµ íŠ¸ë Œë“œ(êµ­ë‚´/í•´ì™¸/êµ­ê°€ë³„)", width="stretch"):
            try:
                st.switch_page("pages/01_naver_datalab_comp.py")
            except Exception:
                st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ 'ë„¤ì´ë²„ DataLab ë¹„êµ íŠ¸ë Œë“œ' í˜ì´ì§€ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        if st.button("ğŸ“ˆ ì¸íŠ¸ë Œë“œ ì–¸ê¸‰ëŸ‰ íŠ¸ë Œë“œ(ë¹ˆë„ìˆ˜)", width="stretch"):
            try:
                st.switch_page("pages/02_sometrend_freq_trend.py")
            except Exception:
                st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ 'ì¸íŠ¸ë Œë“œ ì–¸ê¸‰ëŸ‰ íŠ¸ë Œë“œ(ë¹ˆë„ìˆ˜)' í˜ì´ì§€ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")

    with col2:
        if st.button("ğŸ•¸ï¸ ì¸íŠ¸ë Œë“œ ì—°ê´€ì„± ë¶„ì„", width="stretch"):
            try:
                st.switch_page("pages/03_sometrend_association.py")
            except Exception:
                st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ 'ì¸íŠ¸ë Œë“œ ì—°ê´€ì„± ë¶„ì„' í˜ì´ì§€ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        if st.button("â˜ï¸ ì¸íŠ¸ë Œë“œ ê¸ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ", width="stretch"):
            try:
                st.switch_page("pages/04_sometrend_sentiment_wordcloud.py")
            except Exception:
                st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ 'ì¸íŠ¸ë Œë“œ ê¸ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ' í˜ì´ì§€ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")

    st.stop()
if menu != "ê´€ë ¨ë‰´ìŠ¤":
    st.stop()


# =============================
# ê´€ë ¨ë‰´ìŠ¤(êµ¬ê¸€ì‹œíŠ¸ â†’ ë§í¬ ëª©ë¡)
# =============================

def _extract_spreadsheet_id(url: str) -> str:
    m = re.search(r"/spreadsheets/d/([a-zA-Z0-9-_]+)", url)
    if not m:
        raise ValueError("êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ URLì—ì„œ ë¬¸ì„œ IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return m.group(1)


def _xlsx_export_url(spreadsheet_id: str) -> str:
    return f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=xlsx"


def _is_probably_html(resp: requests.Response) -> bool:
    ctype = (resp.headers.get("Content-Type") or "").lower()
    if "text/html" in ctype:
        return True
    head = resp.content[:200].lstrip().lower()
    return head.startswith(b"<!doctype html") or head.startswith(b"<html")


def _download_with_retry(url: str, timeout_s: int = 30, retries: int = 3, backoff_s: float = 0.8) -> requests.Response:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
    }
    last_exc: Optional[Exception] = None
    for i in range(retries):
        try:
            resp = requests.get(url, headers=headers, timeout=timeout_s, allow_redirects=True)
            resp.raise_for_status()
            if _is_probably_html(resp):
                raise ValueError("ì‘ë‹µì´ HTMLì…ë‹ˆë‹¤(ê¶Œí•œ/ê³µê°œ ì„¤ì • ë¬¸ì œ ê°€ëŠ¥).")
            return resp
        except Exception as e:
            last_exc = e
            time.sleep(backoff_s * (2**i))
    raise RuntimeError(f"ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì‹¤íŒ¨: {last_exc}")


@st.cache_data(show_spinner=False)
def _fetch_news_sheet(sheet_url: str) -> pd.DataFrame:
    sid = _extract_spreadsheet_id(sheet_url)
    resp = _download_with_retry(_xlsx_export_url(sid))
    with BytesIO(resp.content) as bio:
        return pd.read_excel(bio)


def _normalize_news(df: pd.DataFrame) -> pd.DataFrame:
    """
    ê¸°ëŒ€ í˜•íƒœ:
    - Aì—´: ê¸°ì‚¬ ì œëª©
    - Bì—´: URL
    (í—¤ë”ê°€ ì—†ì„ ìˆ˜ë„ ìˆì–´, ì»¬ëŸ¼ëª…ê³¼ ë¬´ê´€í•˜ê²Œ 0/1ì—´ì„ ìš°ì„  ì‚¬ìš©)
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["ì œëª©", "URL"])

    cols = list(df.columns)
    a = cols[0]
    b = cols[1] if len(cols) > 1 else cols[0]
    out = df[[a, b]].copy()
    out.columns = ["ì œëª©", "URL"]

    out["ì œëª©"] = out["ì œëª©"].astype(str).str.strip()
    out["URL"] = out["URL"].astype(str).str.strip()
    out = out[(out["ì œëª©"] != "") & (out["ì œëª©"].str.lower() != "nan")].copy()
    out = out[out["URL"].str.startswith(("http://", "https://"))].copy()
    out = out.drop_duplicates(subset=["URL"]).reset_index(drop=True)
    return out


st.subheader("ğŸ“° ê´€ë ¨ë‰´ìŠ¤")

try:
    news_raw = _fetch_news_sheet(news_sheet_url)
    news_df = _normalize_news(news_raw)
except Exception as e:
    st.error(f"ê´€ë ¨ë‰´ìŠ¤ ì‹œíŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

if news_query.strip():
    q = news_query.strip()
    news_df = news_df[news_df["ì œëª©"].str.contains(q, case=False, na=False)].copy()

news_df = news_df.head(int(max_items)).copy()

if len(news_df) == 0:
    st.info("í‘œì‹œí•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ê²€ìƒ‰ì–´/ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”)")
else:
    st.caption(f"ë‰´ìŠ¤ {len(news_df)}ê±´ Â· ì†ŒìŠ¤: {news_sheet_url}")
    for i, r in news_df.iterrows():
        title = r["ì œëª©"]
        url = r["URL"]
        st.markdown(f"- [{title}]({url})")


